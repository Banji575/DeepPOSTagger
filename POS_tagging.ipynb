{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Download and Read Function\n",
    "This section defines a function to download and read a dataset, specifically handling sentence and part-of-speech (POS) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences: 3914\n",
      "# of POS sequences: 3914\n"
     ]
    }
   ],
   "source": [
    "def download_and_read(dataset_dir, num_pairs=None):\n",
    "    sent_filename = os.path.join(dataset_dir, 'treebank-sents.txt')\n",
    "    pos_filename = os.path.join(dataset_dir, 'treebank-poss.txt')\n",
    "    \n",
    "    if not (os.path.exists(sent_filename) and os.path.exists(pos_filename)):\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "\n",
    "        nltk.download('treebank')\n",
    "        sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "        with open(sent_filename, 'w') as fsents, open(pos_filename, 'w') as fposs:\n",
    "            for sent in sentences:\n",
    "                words, tags = zip(*sent)\n",
    "                fsents.write(' '.join(words) + '\\n')\n",
    "                fposs.write(' '.join(tags) + '\\n')\n",
    "\n",
    "    sents, poss = [], []\n",
    "    with open(sent_filename, 'r') as fsent:\n",
    "        for idx, line in enumerate(fsent):\n",
    "            sents.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs - 1:\n",
    "                break\n",
    "\n",
    "    with open(pos_filename, 'r') as fposs:\n",
    "        for idx, line in enumerate(fposs):\n",
    "            poss.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs - 1:\n",
    "                break\n",
    "\n",
    "    return sents, poss\n",
    "\n",
    "sents, poss = download_and_read('./datasets')\n",
    "print('# of sentences:', len(sents))\n",
    "print('# of POS sequences:', len(poss))\n",
    "assert(len(sents) == len(poss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenization and Vocabulary Building\n",
    "This code segment defines a function for tokenizing text and building a vocabulary, suitable for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes(source): 9001, (target): 39\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_and_build_vocab(texts, vocab_size=None, lower=True):\n",
    "    if vocab_size is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size + 1, oov_token='UNK', lower=lower\n",
    "        )\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    if vocab_size is not None:\n",
    "        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() if i <= vocab_size + 1}\n",
    "    word2indx = tokenizer.word_index\n",
    "    ind2word = {v:k for k, v in word2indx.items()}\n",
    "    return word2indx, ind2word, tokenizer\n",
    "\n",
    "word2ind_s, idx2word_s, tokenizer_s = tokenizer_and_build_vocab(\n",
    "    sents, vocab_size=9000\n",
    ")\n",
    "word2ind_t, idx2word_t, tokenizer_t = tokenizer_and_build_vocab(\n",
    "    poss, vocab_size=38, lower=False\n",
    ")\n",
    "source_vocab_size = len(word2ind_s)\n",
    "target_vocab_size = len(word2ind_t)\n",
    "\n",
    "print('vocab sizes(source): {:d}, (target): {:d}'.format(\n",
    "    source_vocab_size, target_vocab_size\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Displaying Sample Sentences and Sequence Length Statistics\n",
    "This code snippet shows a sample of sentences and calculates statistical percentiles for sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .', 'Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .', 'Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named *-1 a nonexecutive director of this British industrial conglomerate .', 'A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago , researchers reported 0 *T*-1 .', 'The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that *T*-1 show up decades later , researchers said 0 *T*-2 .', 'Lorillard Inc. , the unit of New York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped using crocidolite in its Micronite cigarette filters in 1956 .', \"Although preliminary findings were reported *-2 more than a year ago , the latest results appear in today 's New England Journal of Medicine , a forum likely * to bring new attention to the problem .\", 'A Lorillard spokewoman said , `` This is an old story .', \"We 're talking about years ago before anyone heard of asbestos having any questionable properties .\", \"There is no asbestos in our products now . ''\"]\n",
      "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n"
     ]
    }
   ],
   "source": [
    "print(sents[:10])\n",
    "sequence_lenghts = np.array([len(s.split()) for s in sents])\n",
    "print([(p, np.percentile(sequence_lenghts, p)) for p in [75, 80, 90, 95, 99, 100]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing Text for Model Training\n",
    "This section covers the conversion of sentences and POS tags to integer sequences, padding, and splitting the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 271\n",
    "\n",
    "#Convert sentences to sequence of integer\n",
    "\n",
    "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
    "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sents_as_ints, maxlen = max_seqlen, padding = 'post'\n",
    ")\n",
    "# convert POS tags to sequence of (categorial) integers\n",
    "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
    "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_ints, maxlen = max_seqlen, padding = 'post'\n",
    ")\n",
    "\n",
    "poss_as_catints = []\n",
    "for p in poss_as_ints:\n",
    "    poss_as_catints.append(tf.keras.utils.to_categorical(p,\n",
    "     num_classes = target_vocab_size + 1, dtype = 'int32'))\n",
    "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_catints, maxlen = max_seqlen\n",
    ")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_catints)\n",
    ")\n",
    "idx2word_s[0], idx2word_t[0] = 'PAD', 'PAD'\n",
    "#split into training, validation, and test datasets\n",
    "\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sents) // 3\n",
    "val_size = (len(sents)-test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "#create batches \n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset  =test_dataset.batch(batch_size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### POS Tagging Model Definition and Training\n",
    "This section includes the definition of a POS tagging model using TensorFlow, its compilation, and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pos_tagging_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  1152128   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  multiple                 0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  multiple                 592896    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  multiple                 20520     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,765,544\n",
      "Trainable params: 1,765,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 10s 134ms/step - loss: 1.4295 - accuracy: 0.8804 - masked_accuracy_fn: 0.0782 - val_loss: 0.3196 - val_accuracy: 0.9172 - val_masked_accuracy_fn: 0.4272\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.3279 - accuracy: 0.9213 - masked_accuracy_fn: 0.1229 - val_loss: 0.3129 - val_accuracy: 0.9274 - val_masked_accuracy_fn: 0.1444\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.3183 - accuracy: 0.9258 - masked_accuracy_fn: 0.1726 - val_loss: 0.3028 - val_accuracy: 0.9272 - val_masked_accuracy_fn: 0.2197\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.3013 - accuracy: 0.9238 - masked_accuracy_fn: 0.2173 - val_loss: 0.2921 - val_accuracy: 0.9224 - val_masked_accuracy_fn: 0.2143\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.2706 - accuracy: 0.9234 - masked_accuracy_fn: 0.1561 - val_loss: 0.2572 - val_accuracy: 0.9258 - val_masked_accuracy_fn: 0.1576\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.2497 - accuracy: 0.9287 - masked_accuracy_fn: 0.1658 - val_loss: 0.2684 - val_accuracy: 0.9203 - val_masked_accuracy_fn: 0.1211\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.2479 - accuracy: 0.9283 - masked_accuracy_fn: 0.1662 - val_loss: 0.2386 - val_accuracy: 0.9324 - val_masked_accuracy_fn: 0.1620\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.2444 - accuracy: 0.9308 - masked_accuracy_fn: 0.1950 - val_loss: 0.2433 - val_accuracy: 0.9346 - val_masked_accuracy_fn: 0.2349\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.2397 - accuracy: 0.9343 - masked_accuracy_fn: 0.2351 - val_loss: 0.2264 - val_accuracy: 0.9379 - val_masked_accuracy_fn: 0.2644\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.2319 - accuracy: 0.9369 - masked_accuracy_fn: 0.2640 - val_loss: 0.2304 - val_accuracy: 0.9381 - val_masked_accuracy_fn: 0.2460\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.2267 - accuracy: 0.9391 - masked_accuracy_fn: 0.2948 - val_loss: 0.2245 - val_accuracy: 0.9388 - val_masked_accuracy_fn: 0.3233\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.2157 - accuracy: 0.9415 - masked_accuracy_fn: 0.3229 - val_loss: 0.2010 - val_accuracy: 0.9455 - val_masked_accuracy_fn: 0.3389\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.2067 - accuracy: 0.9432 - masked_accuracy_fn: 0.3492 - val_loss: 0.1973 - val_accuracy: 0.9460 - val_masked_accuracy_fn: 0.4055\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.1925 - accuracy: 0.9472 - masked_accuracy_fn: 0.3858 - val_loss: 0.1796 - val_accuracy: 0.9507 - val_masked_accuracy_fn: 0.4420\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.1870 - accuracy: 0.9497 - masked_accuracy_fn: 0.4259 - val_loss: 0.1805 - val_accuracy: 0.9523 - val_masked_accuracy_fn: 0.4541\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.1792 - accuracy: 0.9525 - masked_accuracy_fn: 0.4600 - val_loss: 0.1751 - val_accuracy: 0.9537 - val_masked_accuracy_fn: 0.4455\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.1693 - accuracy: 0.9555 - masked_accuracy_fn: 0.4908 - val_loss: 0.1449 - val_accuracy: 0.9632 - val_masked_accuracy_fn: 0.5403\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1657 - accuracy: 0.9564 - masked_accuracy_fn: 0.5067 - val_loss: 0.1687 - val_accuracy: 0.9552 - val_masked_accuracy_fn: 0.5060\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.1544 - accuracy: 0.9592 - masked_accuracy_fn: 0.5344 - val_loss: 0.1550 - val_accuracy: 0.9588 - val_masked_accuracy_fn: 0.6243\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.1523 - accuracy: 0.9598 - masked_accuracy_fn: 0.5409 - val_loss: 0.1563 - val_accuracy: 0.9583 - val_masked_accuracy_fn: 0.5969\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.1473 - accuracy: 0.9608 - masked_accuracy_fn: 0.5566 - val_loss: 0.1346 - val_accuracy: 0.9647 - val_masked_accuracy_fn: 0.5986\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.1391 - accuracy: 0.9627 - masked_accuracy_fn: 0.5735 - val_loss: 0.1369 - val_accuracy: 0.9633 - val_masked_accuracy_fn: 0.6481\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.1354 - accuracy: 0.9635 - masked_accuracy_fn: 0.5875 - val_loss: 0.1226 - val_accuracy: 0.9663 - val_masked_accuracy_fn: 0.6334\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1292 - accuracy: 0.9648 - masked_accuracy_fn: 0.5976 - val_loss: 0.1220 - val_accuracy: 0.9666 - val_masked_accuracy_fn: 0.6554\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.1256 - accuracy: 0.9656 - masked_accuracy_fn: 0.6106 - val_loss: 0.1204 - val_accuracy: 0.9669 - val_masked_accuracy_fn: 0.6304\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.1212 - accuracy: 0.9663 - masked_accuracy_fn: 0.6133 - val_loss: 0.1258 - val_accuracy: 0.9642 - val_masked_accuracy_fn: 0.6119\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.1175 - accuracy: 0.9671 - masked_accuracy_fn: 0.6249 - val_loss: 0.1126 - val_accuracy: 0.9680 - val_masked_accuracy_fn: 0.6161\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.1146 - accuracy: 0.9675 - masked_accuracy_fn: 0.6330 - val_loss: 0.1109 - val_accuracy: 0.9685 - val_masked_accuracy_fn: 0.6085\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.1101 - accuracy: 0.9687 - masked_accuracy_fn: 0.6388 - val_loss: 0.1106 - val_accuracy: 0.9686 - val_masked_accuracy_fn: 0.7178\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.1063 - accuracy: 0.9697 - masked_accuracy_fn: 0.6479 - val_loss: 0.0988 - val_accuracy: 0.9719 - val_masked_accuracy_fn: 0.6748\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.1082 - accuracy: 0.9689 - masked_accuracy_fn: 0.6510 - val_loss: 0.1058 - val_accuracy: 0.9698 - val_masked_accuracy_fn: 0.6260\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.1034 - accuracy: 0.9702 - masked_accuracy_fn: 0.6587 - val_loss: 0.1052 - val_accuracy: 0.9690 - val_masked_accuracy_fn: 0.6534\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.1007 - accuracy: 0.9708 - masked_accuracy_fn: 0.6642 - val_loss: 0.0984 - val_accuracy: 0.9709 - val_masked_accuracy_fn: 0.6587\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.0975 - accuracy: 0.9717 - masked_accuracy_fn: 0.6774 - val_loss: 0.0917 - val_accuracy: 0.9741 - val_masked_accuracy_fn: 0.6596\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.0984 - accuracy: 0.9715 - masked_accuracy_fn: 0.6782 - val_loss: 0.0929 - val_accuracy: 0.9726 - val_masked_accuracy_fn: 0.6900\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.0932 - accuracy: 0.9731 - masked_accuracy_fn: 0.6933 - val_loss: 0.0924 - val_accuracy: 0.9727 - val_masked_accuracy_fn: 0.7570\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.0916 - accuracy: 0.9733 - masked_accuracy_fn: 0.6980 - val_loss: 0.0925 - val_accuracy: 0.9723 - val_masked_accuracy_fn: 0.6385\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.0918 - accuracy: 0.9731 - masked_accuracy_fn: 0.6961 - val_loss: 0.0889 - val_accuracy: 0.9738 - val_masked_accuracy_fn: 0.7252\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.0873 - accuracy: 0.9746 - masked_accuracy_fn: 0.7115 - val_loss: 0.0808 - val_accuracy: 0.9765 - val_masked_accuracy_fn: 0.7094\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.0871 - accuracy: 0.9746 - masked_accuracy_fn: 0.7119 - val_loss: 0.0779 - val_accuracy: 0.9775 - val_masked_accuracy_fn: 0.7390\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.0830 - accuracy: 0.9757 - masked_accuracy_fn: 0.7268 - val_loss: 0.0756 - val_accuracy: 0.9778 - val_masked_accuracy_fn: 0.8011\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.0820 - accuracy: 0.9758 - masked_accuracy_fn: 0.7271 - val_loss: 0.0783 - val_accuracy: 0.9771 - val_masked_accuracy_fn: 0.7925\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.0805 - accuracy: 0.9762 - masked_accuracy_fn: 0.7314 - val_loss: 0.0736 - val_accuracy: 0.9789 - val_masked_accuracy_fn: 0.7697\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.0795 - accuracy: 0.9766 - masked_accuracy_fn: 0.7339 - val_loss: 0.0692 - val_accuracy: 0.9795 - val_masked_accuracy_fn: 0.7528\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.0762 - accuracy: 0.9776 - masked_accuracy_fn: 0.7470 - val_loss: 0.0756 - val_accuracy: 0.9775 - val_masked_accuracy_fn: 0.8328\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.0778 - accuracy: 0.9771 - masked_accuracy_fn: 0.7413 - val_loss: 0.0697 - val_accuracy: 0.9794 - val_masked_accuracy_fn: 0.7776\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.0738 - accuracy: 0.9780 - masked_accuracy_fn: 0.7519 - val_loss: 0.0687 - val_accuracy: 0.9807 - val_masked_accuracy_fn: 0.7543\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.0741 - accuracy: 0.9778 - masked_accuracy_fn: 0.7488 - val_loss: 0.0686 - val_accuracy: 0.9796 - val_masked_accuracy_fn: 0.7872\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.0708 - accuracy: 0.9789 - masked_accuracy_fn: 0.7610 - val_loss: 0.0607 - val_accuracy: 0.9815 - val_masked_accuracy_fn: 0.8225\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.0706 - accuracy: 0.9788 - masked_accuracy_fn: 0.7591 - val_loss: 0.0699 - val_accuracy: 0.9786 - val_masked_accuracy_fn: 0.7532\n"
     ]
    }
   ],
   "source": [
    "class POSTaggingModel(tf.keras.Model):\n",
    "    def __init__ (self, source_vocab_size, target_vocab_size,\n",
    "                  embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
    "        super(POSTaggingModel, self).__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(\n",
    "            source_vocab_size, embedding_dim, input_length = max_seqlen\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(.2)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(rnn_output_dim, return_sequences = True)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(target_vocab_size)\n",
    "        )\n",
    "        self.activation = tf.keras.layers.Activation('softmax')\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "def masked_accuracy():\n",
    "    def masked_accuracy_fn(ytrue, ypred):\n",
    "                ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
    "                ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
    "                mask = tf.keras.backend.cast(\n",
    "                    tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
    "                matches = tf.keras.backend.cast(\n",
    "                    tf.keras.backend.equal(ytrue, ypred), tf.int32\n",
    "                )* mask\n",
    "                numer = tf.keras.backend.sum(matches)\n",
    "                denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask),1)\n",
    "                accuracy = numer/denom\n",
    "                return accuracy\n",
    "    return masked_accuracy_fn\n",
    "num_epochs = 50\n",
    "best_model_file = os.path.join('./datasets', 'best_model.h5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            best_model_file, \n",
    "            save_weights_only = True,\n",
    "            save_best_only = True\n",
    "        )\n",
    "\n",
    "embedding_dim = 128\n",
    "rnn_output_dim = 256\n",
    "\n",
    "model = POSTaggingModel(source_vocab_size, target_vocab_size + 1,\n",
    "                                embedding_dim, max_seqlen, rnn_output_dim)\n",
    "        \n",
    "model.build(input_shape = (batch_size, max_seqlen))\n",
    "model.summary()\n",
    "model.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer = 'adam',\n",
    "            metrics = ['accuracy', masked_accuracy()]\n",
    "        )\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir = './log_dir')\n",
    "history = model.fit(train_dataset, epochs = num_epochs,\n",
    "                            validation_data = val_dataset, callbacks = [checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
